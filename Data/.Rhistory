as.integer(factor(x))
15/7
-(4/3)
-(4/5)
-(4/5)*5
-4/5*5
-2*-10.5
-11/8
-1.375*8
-1*-1
-1*-.3
contr.poly()
contr.poly(3)
M <- matrix(1,3,4,5)
<
M
t(m)
t(M)
## Simulate Bayesian Binomial updating
sim_bayes<-function(p=0.5,N=10,y_lim=15)
{
success<-0
curve(dbeta(x,1,1),xlim=c(0,1),ylim=c(0,y_lim),xlab='p',ylab='Posterior Density',lty=2)
legend('topright',legend=c('Prior','Updated Posteriors','Final Posterior'),lty=c(2,1,1),col=c('black','black','red'))
for(i in 1:N)
{
if(runif(1,0,1)<=p)
success<-success+1
curve(dbeta(x,success+1,(i-success)+1),add=TRUE)
print(paste(success,"successes and ",i-success," failures"))
}
curve(dbeta(x,success+1,(i-success)+1),add=TRUE,col='red',lwd=1.5)
}
sim_bayes(p=0.6,N=90)
library(dplyr)
num_trials <- 10e6
simulations <- data_frame(
true_average = rbeta(num_trials, 81, 219),
hits = rbinom(num_trials, 300, true_average)
)
simulations <- tibble(
true_average = rbeta(num_trials, 81, 219),
hits = rbinom(num_trials, 300, true_average)
)
simulations
hit_100 <- simulations %>%
filter(hits == 100)
hist(hit_100$true_average)
#Visualise the results with a histogram
hist(hit_100$true_average, density = T)
#Visualise the results with a histogram
hist(hit_100$true_average, density = TRUE)
#Visualise the results with a histogram
hist(hit_100$true_average, density = TRUE)
#Visualise the results with a histogram
hist(hit_100$true_average)
simulations %>%
filter(hits %in% c(60, 80, 100)) %>%
ggplot(aes(true_average, color = factor(hits))) +
geom_density() +
labs(x = "True average of players with H hits / 300 at-bats",
color = "H")
library(ggplot2)
simulations %>%
filter(hits %in% c(60, 80, 100)) %>%
ggplot(aes(true_average, color = factor(hits))) +
geom_density() +
labs(x = "True average of players with H hits / 300 at-bats",
color = "H")
library(tidyverse)
library(Lahman)
install.packages("Lahman")
library(Lahman)
career <- Batting %>%
filter(AB > 0) %>%
anti_join(Pitching, by = "playerID") %>%
group_by(playerID) %>%
summarize(H = sum(H), AB = sum(AB)) %>%
mutate(average = H / AB)
career <- Master %>%
tbl_df() %>%
dplyr::select(playerID, nameFirst, nameLast) %>%
unite(name, nameFirst, nameLast, sep = " ") %>%
inner_join(career, by = "playerID") %>%
dplyr::select(-playerID)
career <- Master %>%
tibble::as_tibble() %>%
dplyr::select(playerID, nameFirst, nameLast) %>%
unite(name, nameFirst, nameLast, sep = " ") %>%
inner_join(career, by = "playerID") %>%
dplyr::select(-playerID)
View(career)
View(career)
nameFirst
career <- Master %>%
tibble::as_tibble() %>%
dplyr::select(playerID, nameFirst, nameLast) %>%
unite(name, nameFirst, nameLast, sep = " ") %>%
inner_join(career, by = "playerID") %>%
dplyr::select(-playerID)
Batting
career <- Batting %>%
filter(AB > 0) %>%
anti_join(Pitching, by = "playerID") %>%
group_by(playerID) %>%
summarize(H = sum(H), AB = sum(AB)) %>%
mutate(average = H / AB)
Master
career <- Master %>%
tibble::as_tibble() %>%
dplyr::select(playerID, nameFirst, nameLast) %>%
unite(name, nameFirst, nameLast, sep = " ") %>%
inner_join(career, by = "playerID") %>%
dplyr::select(-playerID)
View(career)
num_trials <- 10e6
simulations <- tibble(
true_average = rbeta(num_trials, 81, 219),
hits = rbinom(num_trials, 300, true_average)
)
simulations
hit_100 <- simulations %>%
filter(hits == 100)
#Visualise the results with a histogram
hist(hit_100$true_average)
simulations %>%
filter(hits %in% c(60, 80, 100)) %>%
ggplot(aes(true_average, color = factor(hits))) +
geom_density() +
labs(x = "True average of players with H hits / 300 at-bats",
color = "H")
career <- Batting %>%
filter(AB > 0) %>%
anti_join(Pitching, by = "playerID") %>%
group_by(playerID) %>%
summarize(H = sum(H), AB = sum(AB)) %>%
mutate(average = H / AB)
career <- Master %>%
tibble::as_tibble() %>%
dplyr::select(playerID, nameFirst, nameLast) %>%
unite(name, nameFirst, nameLast, sep = " ") %>%
inner_join(career, by = "playerID") %>%
dplyr::select(-playerID)
View(career)
career[max(),]
career$average
career$average %>% max()
career$average %>% min()
library(stats4)
career_filtered <- career %>%
filter(AB > 500)
# log-likelihood function
ll <- function(alpha, beta) {
x <- career_filtered$H
total <- career_filtered$AB
-sum(VGAM::dbetabinom.ab(x, total, alpha, beta, log = TRUE))
}
m <- mle(ll, start = list(alpha = 1, beta = 10), method = "L-BFGS-B",
lower = c(0.0001, .1))
ab <- coef(m)
alpha0 <- ab[1]
beta0 <- ab[2]
# log-likelihood function
ll <- function(alpha, beta) {
x <- career_filtered$H
total <- career_filtered$AB
-sum(VGAM::dbetabinom.ab(x, total, alpha, beta, log = TRUE))
}
m <- mle(ll, start = list(alpha = 1, beta = 10), method = "L-BFGS-B",
lower = c(0.0001, .1))
m <- mle(ll, start = list(alpha = 1, beta = 10), method = "L-BFGS-B",
lower = c(0.0001, .1))
library(tidyverse)
library(ggplot2)
install.packages("Lahman")
library(Lahman)
num_trials <- 10e6
simulations <- tibble(
true_average = rbeta(num_trials, 81, 219),
hits = rbinom(num_trials, 300, true_average)
)
simulations
hit_100 <- simulations %>%
filter(hits == 100)
#Visualise the results with a histogram
hist(hit_100$true_average)
simulations %>%
filter(hits %in% c(60, 80, 100)) %>%
ggplot(aes(true_average, color = factor(hits))) +
geom_density() +
labs(x = "True average of players with H hits / 300 at-bats",
color = "H")
#Chapter 3 Bayesian estimation, binomal mdoel.
career <- Batting %>%
filter(AB > 0) %>%
anti_join(Pitching, by = "playerID") %>%
group_by(playerID) %>%
summarize(H = sum(H), AB = sum(AB)) %>%
mutate(average = H / AB)
career <- Master %>%
tibble::as_tibble() %>%
dplyr::select(playerID, nameFirst, nameLast) %>%
unite(name, nameFirst, nameLast, sep = " ") %>%
inner_join(career, by = "playerID") %>%
dplyr::select(-playerID)
career$average %>% min()
#Estimtate prior from data. EB is an approximation to Full
# bayesian methods which becomes more accuracte as n increases.
library(stats4)
career_filtered <- career %>%
filter(AB > 500)
# log-likelihood function
ll <- function(alpha, beta) {
x <- career_filtered$H
total <- career_filtered$AB
-sum(VGAM::dbetabinom.ab(x, total, alpha, beta, log = TRUE))
}
m <- mle(ll, start = list(alpha = 1, beta = 10), method = "L-BFGS-B",
lower = c(0.0001, .1))
ab <- coef(m)
alpha0 <- ab[1]
beta0 <- ab[2]
install.packages("Lahman")
num_trials <- 10e6
simulations <- tibble(
true_average = rbeta(num_trials, 81, 219),
hits = rbinom(num_trials, 300, true_average)
)
simulations
hit_100 <- simulations %>%
filter(hits == 100)
#Visualise the results with a histogram
hist(hit_100$true_average)
simulations %>%
filter(hits %in% c(60, 80, 100)) %>%
ggplot(aes(true_average, color = factor(hits))) +
geom_density() +
labs(x = "True average of players with H hits / 300 at-bats",
color = "H")
career <- Batting %>%
filter(AB > 0) %>%
anti_join(Pitching, by = "playerID") %>%
group_by(playerID) %>%
summarize(H = sum(H), AB = sum(AB)) %>%
mutate(average = H / AB)
career <- Master %>%
tibble::as_tibble() %>%
dplyr::select(playerID, nameFirst, nameLast) %>%
unite(name, nameFirst, nameLast, sep = " ") %>%
inner_join(career, by = "playerID") %>%
dplyr::select(-playerID)
career$average %>% min()
library(stats4)
career_filtered <- career %>%
filter(AB > 500)
# log-likelihood function
ll <- function(alpha, beta) {
x <- career_filtered$H
total <- career_filtered$AB
-sum(VGAM::dbetabinom.ab(x, total, alpha, beta, log = TRUE))
}
m <- mle(ll, start = list(alpha = 1, beta = 10), method = "L-BFGS-B",
lower = c(0.0001, .1))
ab <- coef(m)
alpha0 <- ab[1]
beta0 <- ab[2]
dbetabinom.ab
undebug(ls)
(101.2-100)/21.2
x <- matrix(c( .8, 102,
1.0,  98,
.5, 100,
.9, 105,
.7, 103,
.4, 110,
1.2,  99,
1.4,  87,
.6, 113,
1.1,  89,
1.3,  93,
.8, 102,
1.0,  98,
.5, 100,
.9, 105,
.7, 103,
.4, 110,
1.2,  99,
1.4,  87,
.6, 113,
1.1,  89,
1.3,  93), nrow=22,ncol=2,byrow=T)
View(x)
plot(x[,1],x[,2])
# clears workspace:
rm(list=ls())
library(rstan)
#### Notes to Stan model #######################################################
## 1) Multivariate normal distribution in Stan uses covariance matrix instead of
##    precision matrix.
## 2) Multivariate normal distribution can be (and is) also vectorized.
## 3) Warnings may occur during sampling, ignore them.
################################################################################
model <- "
// Pearson Correlation
data {
int<lower=0> n;
vector[2] x[n];
}
parameters {
vector[2] mu;
vector<lower=0>[2] lambda;
real<lower=-1,upper=1> r;
}
transformed parameters {
vector<lower=0>[2] sigma;
cov_matrix[2] T;
// Reparameterization
sigma[1] <- inv_sqrt(lambda[1]);
sigma[2] <- inv_sqrt(lambda[2]);
T[1,1] <- square(sigma[1]);
T[1,2] <- r * sigma[1] * sigma[2];
T[2,1] <- r * sigma[1] * sigma[2];
T[2,2] <- square(sigma[2]);
}
model {
// Priors
mu ~ normal(0, inv_sqrt(.001));
lambda ~ gamma(.001, .001);
// Data
x ~ multi_normal(mu, T);
}"
# Choose a dataset:
dataset <- 1
# The datasets:
if (dataset == 1) {
x <- matrix(c( .8, 102,
1.0,  98,
.5, 100,
.9, 105,
.7, 103,
.4, 110,
1.2,  99,
1.4,  87,
.6, 113,
1.1,  89,
1.3,  93), nrow=11, ncol=2, byrow=T)
}
if (dataset == 2) {
x <- matrix(c( .8, 102,
1.0,  98,
.5, 100,
.9, 105,
.7, 103,
.4, 110,
1.2,  99,
1.4,  87,
.6, 113,
1.1,  89,
1.3,  93,
.8, 102,
1.0,  98,
.5, 100,
.9, 105,
.7, 103,
.4, 110,
1.2,  99,
1.4,  87,
.6, 113,
1.1,  89,
1.3,  93), nrow=22,ncol=2,byrow=T)
}
n <- nrow(x) # number of people/units measured
data <- list(x=x, n=n) # to be passed on to Stan
myinits <- list(
list(r=0, mu=c(0, 0), lambda=c(1, 1)))
# parameters to be monitored:
parameters <- c("r", "mu", "sigma")
# The following command calls Stan with specific options.
# For a detailed description type "?rstan".
samples <- stan(model_code=model,
data=data,
init=myinits,  # If not specified, gives random inits
pars=parameters,
iter=10000,
chains=1,
thin=1,
# warmup = 100,  # Stands for burn-in; Default = iter/2
# seed = 123  # Setting seed; Default is random seed
)
r <- extract(samples)$r
#Frequentist point-estimate of r:
freq.r <- cor(x[,1],x[,2])
#make the two panel plot:
windows(width=9,height=6) #this command works only under Windows!
layout(matrix(c(1,2),1,2))
layout.show(2)
#some plotting options to make things look better:
par(cex.main=1.5, mar=c(5, 6, 4, 5) + 0.1, mgp=c(3.5, 1, 0), cex.lab=1.5,
font.lab=2, cex.axis=1.3, bty = "n", las=1)
# data panel:
plot(x[,1],x[,2], type="p", pch=19, cex=1)
# correlation panel:
plot(density(r, from=-1,to=1), main="", ylab="Posterior Density",
xlab="Correlation", lwd=2)
lines(c(freq.r, freq.r), c(0,100), lwd=2, lty=2)
# clears workspace:
rm(list=ls())
library(rstan)
#### Notes to Stan model #######################################################
## 1) Multivariate normal distribution in Stan uses covariance matrix instead of
##    precision matrix.
## 2) Multivariate normal distribution can be (and is) also vectorized.
## 3) Warnings may occur during sampling, ignore them.
################################################################################
model <- "
// Pearson Correlation
data {
int<lower=0> n;
vector[2] x[n];
}
parameters {
vector[2] mu;
vector<lower=0>[2] lambda;
real<lower=-1,upper=1> r;
}
transformed parameters {
vector<lower=0>[2] sigma;
cov_matrix[2] T;
// Reparameterization
sigma[1] <- inv_sqrt(lambda[1]);
sigma[2] <- inv_sqrt(lambda[2]);
T[1,1] <- square(sigma[1]);
T[1,2] <- r * sigma[1] * sigma[2];
T[2,1] <- r * sigma[1] * sigma[2];
T[2,2] <- square(sigma[2]);
}
model {
// Priors
mu ~ normal(0, inv_sqrt(.001));
lambda ~ gamma(.001, .001);
// Data
x ~ multi_normal(mu, T);
}"
# Choose a dataset:
dataset <- 1
# The datasets:
if (dataset == 1) {
x <- matrix(c( .8, 102,
1.0,  98,
.5, 100,
.9, 105,
.7, 103,
.4, 110,
1.2,  99,
1.4,  87,
.6, 113,
1.1,  89,
1.3,  93), nrow=11, ncol=2, byrow=T)
}
if (dataset == 2) {
x <- matrix(c( .8, 102,
1.0,  98,
.5, 100,
.9, 105,
.7, 103,
.4, 110,
1.2,  99,
1.4,  87,
.6, 113,
1.1,  89,
1.3,  93,
.8, 102,
1.0,  98,
.5, 100,
.9, 105,
.7, 103,
.4, 110,
1.2,  99,
1.4,  87,
.6, 113,
1.1,  89,
1.3,  93), nrow=22,ncol=2,byrow=T)
}
n <- nrow(x) # number of people/units measured
data <- list(x=x, n=n) # to be passed on to Stan
myinits <- list(
list(r=0, mu=c(0, 0), lambda=c(1, 1)))
# parameters to be monitored:
parameters <- c("r", "mu", "sigma")
# The following command calls Stan with specific options.
# For a detailed description type "?rstan".
samples <- stan(model_code=model,
data=data,
init=myinits,  # If not specified, gives random inits
pars=parameters,
iter=10000,
chains=1,
thin=1,
# warmup = 100,  # Stands for burn-in; Default = iter/2
# seed = 123  # Setting seed; Default is random seed
)
print(samples)
setwd("~/Repositories/Statistical-methods-for-research-workers-bayes-for-psychologists-and-neuroscientists/
/Data")
setwd("~/Repositories/Statistical-methods-for-research-workers-bayes-for-psychologists-and-neuroscientists/
Data")
setwd("~/Repositories/Statistical-methods-for-research-workers-bayes-for-psychologists-and-neuroscientists/Data")
df <- read.csv(file= "Crime.csv")
df <- read.csv(file= "Crime.csv")
df <- read.csv(file= "Crime.csv",header = T)
View(df)
cor(df$Ã¯..CrimeRate, df$BelowWage10)
